{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e327616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Ridge Regression vs. Ordinary Least Squares Regression:\n",
    "\n",
    "# Ridge Regression is a regularization technique used to prevent overfitting by\n",
    "# adding a penalty term to the least squares loss function.\n",
    "# In ordinary least squares (OLS) regression, the objective is to minimize the \n",
    "# sum of squared residuals without any penalty term.\n",
    "# Ridge Regression adds a penalty term proportional to the square of the\n",
    "# magnitude of the coefficients, which shrinks the coefficients towards zero\n",
    "# but does not set them exactly to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2145e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Assumptions of Ridge Regression:\n",
    "\n",
    "# Same as ordinary least squares regression, Ridge Regression assumes linearity \n",
    "# between the predictors and the response variable.\n",
    "# It also assumes that the residuals are normally distributed with constant\n",
    "# variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e64fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Selection of Tuning Parameter (Lambda):\n",
    "\n",
    "# The tuning parameter, lambda (Î»), controls the strength of the penalty in \n",
    "# Ridge Regression.\n",
    "# It can be selected using techniques like cross-validation, where different \n",
    "# values of lambda are tried, and the one that minimizes the validation error \n",
    "# is chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "530c9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Ridge Regression for Feature Selection:\n",
    "\n",
    "# Ridge Regression can indirectly be used for feature selection by shrinking \n",
    "# the coefficients of less important variables towards zero.\n",
    "# However, it does not perform variable selection in the same way as methods\n",
    "# like Lasso Regression, which can set coefficients exactly to zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16865363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Performance in the Presence of Multicollinearity:\n",
    "\n",
    "# Ridge Regression is effective in handling multicollinearity, a situation \n",
    "# where predictor variables are highly correlated.\n",
    "# By shrinking the coefficients, it reduces the impact of multicollinearity \n",
    "# on the model's stability and performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e66afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Handling of Categorical and Continuous Variables:\n",
    "\n",
    "# Ridge Regression can handle both categorical and continuous independent\n",
    "# variables.\n",
    "# Categorical variables need to be appropriately encoded before applying \n",
    "# Ridge Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c643c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Interpretation of Ridge Regression Coefficients:\n",
    "\n",
    "# The coefficients in Ridge Regression represent the relationship between \n",
    "# each predictor variable and the response variable, considering the presence \n",
    "# of all other predictors in the model.\n",
    "# Higher magnitude coefficients indicate stronger relationships with the\n",
    "# response variable, but their interpretation becomes less straightforward \n",
    "# due to the regularization penalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537bb334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. Use in Time-Series Data Analysis:\n",
    "\n",
    "# Ridge Regression can be adapted for time-series data analysis by incorporating\n",
    "# lagged values of the target variable and other relevant predictors.\n",
    "# The regularization helps in stabilizing parameter estimates and reducing \n",
    "# overfitting, making it suitable for time-series forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ac859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
